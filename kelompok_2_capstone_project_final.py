# -*- coding: utf-8 -*-
"""Kelompok_2_Capstone_Project_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15eG90_lb0W8P7ZxjvSkn9U7mOgzYKX8-

# Capstone Project NLP - Kelompok 2
Anggota Kelompok :
- Dhiwa Aqsha - Universitas Tarumanagara - Teknik Informatika - MSIB Batch 6
- Disky Phiter Budiyansyah - Universitas Krisnadwipayana - Teknik Informatika - MSIB Batch 6
- Devi Andiska P - Universitas Amikom Yogyakarta - Sistem Informasi - MSIB Batch 6
- Dithyo Danoetirto Wahid - Universitas Dinamika - Teknik Komputer - MSIB Batch 6
- Fa'iq Rindha Maulana - Universitas Diponegoro - Informatika - MSIB Batch 6
- Defin Surjaniah - institut teknologi Sumatera - teknik informatika - MSIB Batch 6

### Problem Statement

Masalah yang ingin saya selesaikan adalah mengidentifikasi job posting asli dan palsu. Dengan meningkatnya penggunaan platform daring untuk mencari pekerjaan, munculnya iklan lowongan kerja palsu menjadi ancaman serius bagi para pencari kerja. Job posting palsu ini dapat menyebabkan kerugian finansial dan emosional bagi para pelamar yang tertipu. Oleh karena itu, diperlukan suatu model yang mampu membedakan antara job posting asli dan palsu secara otomatis dan akurat.

### Prediksi apakah sebuah lowongan pekerjaan termasuk palsu atau asli
Project ini akan membuat model klasifikasi yang dapat mempelajari dan mengindetifikasi deskripsi pekerjaan yang bersifat penipuan.\
Kami akan menggunakan model LSTM (Long short-term memory) untuk kasus prediksi.

### Background

Pentingnya mengidentifikasi job posting palsu didasarkan pada beberapa faktor:
1.   **Meningkatnya Penipuan**: Seiring dengan bertambahnya penggunaan platform daring untuk mencari pekerjaan, jumlah penipuan job posting juga meningkat. Penipu memanfaatkan ketidakmampuan pencari kerja untuk memverifikasi keaslian iklan lowongan.
2.   **Dampak Negatif pada Pelamar Kerja**: Job posting palsu dapat menyebabkan kerugian finansial, seperti biaya yang dikeluarkan untuk aplikasi palsu, serta dampak psikologis yang merugikan, seperti stres dan kehilangan kepercayaan diri.
3.   **Kebutuhan untuk Solusi Otomatis**: Mengingat volume data yang besar, solusi manual untuk memverifikasi keaslian job posting tidak efisien dan memakan waktu. Oleh karena itu, dibutuhkan model otomatis yang dapat melakukan tugas ini dengan cepat dan akurat.

### Overview of Dataset
Dataset terdiri dari 18 ribu data yang berisi deskripsi pekerjaan.\
Dataset dapat diidentifikasi palsu atau penipuan melalui kolom "fraudulent".\
Dataset diambil dari website kaggle dengan format csv. Klik [disini](https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction/data) untuk melihat dataset.\
Berikut merupakan penjelasan dari setiap kolum:
- title: Judul atau nama posisi pekerjaan yang diiklankan.
- location: Lokasi di mana pekerjaan tersebut berada atau tersedia.
- department: Departemen atau divisi spesifik dalam organisasi tempat posisi pekerjaan tersebut berada.
- salary_range: Rentang gaji yang ditawarkan.
- company_profile: Deskripsi perusahaan yang merekrut, termasuk latar belakangnya, misi, nilai-nilai, produk, dan layanan.
- description: Deskripsi detail tentang peran pekerjaan, tanggung jawab, tugas, dan kualifikasi yang diperlukan.
- requirements: Persyaratan atau kualifikasi spesifik yang harus dipenuhi kandidat untuk dipertimbangkan dalam pekerjaan, seperti pendidikan, pengalaman, keterampilan, sertifikasi, dll.
- benefits: Manfaat tambahan atau keuntungan yang ditawarkan kepada karyawan, seperti asuransi kesehatan, rencana pensiun, hari libur, dll.
- telecommuting: Menunjukkan apakah pekerjaan menawarkan opsi bekerja dari jarak jauh atau remote (1 untuk ya, 0 untuk tidak).
- has_company_logo: Menunjukkan apakah perusahaan memiliki logo atau tidak (1 untuk ya, 0 untuk tidak).
- has_questions: Apakah posting pekerjaan memiliki pertanyaan skrining untuk pelamar (1 untuk ya, 0 untuk tidak).
- employment_type: Jenis pekerjaan yang ditawarkan untuk posisi pekerjaan, seperti penuh waktu, paruh waktu, kontrak, sementara, dll.
- required_experience: Tingkat pengalaman yang diperlukan untuk peran pekerjaan, seperti pemula, menengah, senior, dll.
- required_education: Tingkat pendidikan minimum yang diperlukan untuk pekerjaan, seperti sekolah menengah atas, gelar sarjana, gelar magister, dll.
- industry: Sektor industri dari perusahaan. seperti teknologi, keuangan, perawatan kesehatan, dll.
- function: Fungsi spesifik atau area spesialisasi dalam industri tersebut, seperti penjualan, pemasaran, rekayasa, dll.
- fraudulent: Menunjukkan apakah posting pekerjaan ditandai sebagai penipuan (1 untuk ya, 0 untuk tidak).

## Import Library
"""

# Import Libraries

import re, string
import nltk
from string import punctuation

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from wordcloud import WordCloud, STOPWORDS

import tensorflow as tf
import tensorflow_hub as tf_hub
from tensorflow.keras.layers import Embedding, TextVectorization, Dense, LSTM, Bidirectional, GRU, Dropout, Reshape
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from keras.callbacks import EarlyStopping

from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from textblob import Word


from warnings import filterwarnings
filterwarnings('ignore')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

"""## Data Loading and Cleaning"""

!mkdir ~/.kaggle

!cp '/content/kaggle.json' ~/.kaggle

!kaggle datasets download -d shivamb/real-or-fake-fake-jobposting-prediction

!unzip /content/real-or-fake-fake-jobposting-prediction.zip

df = pd.read_csv('/content/real-or-fake-fake-jobposting-prediction.zip')
df.head()

"""> Masih banyak kolum yang tidak dibutuhkan pada kasus ini, beberapa kolum akan dihapus dan digabungkan menjadi 1 kolum saja."""

df.info()

df.describe()

"""> Terdapat 5 kolom berbentuk numerik, kita dapat menghapus kolom tersebut karena tidak digunakan untuk klasifikasi teks NLP.
>
> Kolum fraudulent tidak perlu dihapus karena merupakan kelas target.
"""

# Menghapus kolum yang tidak dibutuhkan
df = df.drop(columns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'salary_range', 'employment_type'])

df.head()

# Melihat kolum apa saja yang ada pada dataset
df.columns

# Check null
df.isna().sum()

"""Seperti yang kita lihat terdapat banyak sekali nilai null pada dataset, maka dari itu kita dapat mengisi nilai null tersebut dengan spasi."""

df.fillna(' ', inplace=True)

df.isna().sum()

"""Sudah tidak ada data yang null"""

# Check data yang duplikat
df.duplicated().sum()

"""Dataset memiliki data yang duplikat, oleh karena itu kita bisa menghapus data duplikat agar lebih bersih"""

# Menghapus data duplikat
df = df.drop_duplicates()
df.duplicated().sum()

df.shape

"""## Visualization"""

plt.figure(figsize=(8, 6))
sns.barplot(x=df['fraudulent'].value_counts().index, y=df['fraudulent'].value_counts().values)
plt.xlabel('Fraudulent')
plt.ylabel('Count')
plt.title('Counts of Fraudulent vs Non-Fraudulent')
plt.show()

df.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)

"""Distribusi data 'fraudulent', dapat kita lihat dari visualisasi di atas bahwa nilai '0' atau bukan penipuan lebih banyak dibanding '1' atau fraud."""

plt.figure(figsize = (10,5))
sns.barplot(x = df.title.value_counts()[:5].index, y = df.title.value_counts()[:5] )
plt.title("Most Common Title of Posts", fontdict = {"fontsize" : 14})
plt.xlabel("Title", fontdict = {"fontsize" : 14})
plt.ylabel("Count", fontdict = {"fontsize" : 14})
plt.xticks(rotation=90);

"""Dari visualisasi di atas, pekerjaan yang paling banyak dicari adalah guru bahasa inggris"""

def split(location):
    l = location.split(',')
    return l[0]

df['country'] = df.location.apply(split)
country = dict(df.country.value_counts()[:11])
del country[' ']
plt.figure(figsize=(10,5))
plt.title('Most job posted by country', fontdict = {"fontsize" : 14})
plt.bar(country.keys(), country.values())
plt.ylabel('No. of jobs', fontdict = {"fontsize" : 14})
plt.xlabel('Countries', fontdict = {"fontsize" : 14})

"""Negara dengan postingan pekerjaan terbanyak adalah Amerika Serikat."""

experience = dict(df.required_experience.value_counts())
del experience[' ']
plt.figure(figsize=(13,5))
plt.bar(experience.keys(), experience.values())
plt.title('Most type of experience is required', fontdict = {"fontsize" : 14})
plt.xlabel('Experience', fontdict = {"fontsize" : 14})
plt.ylabel('no. of jobs', fontdict = {"fontsize" : 14})
plt.show()

"""Tingkat pengalaman yang paling banyak dibutuhkan adalah Middle senior level.

## EDA & Text Preporcessing
"""

df['combined_text'] = df['title']+' '+df['location']+' '+df['company_profile']+' '+df['description']+' '+df['requirements']+' '+df['benefits']+' '+df['industry']
df = df.drop(columns=['title', 'location', 'department', 'description', 'company_profile', 'requirements', 'benefits', 'required_experience', 'required_education', 'industry', 'function', 'country'])

"""Sebelum kita melakukan EDA dan Cleaning data, kita harus menggabungkan setiap kolum yang akan digunakan menjadi satu kolum."""

df.head()

"""Seperti kita lihat hanya terdapat 2 kolu yaitu 'fraudulent' sebagai kelas target dan 'combined_text'. Kolum lain yang sudah tidak dipakai dapat di hapus atau di hilangkan"""

fraudjobs_text_dirty = df[df.fraudulent==1].combined_text
actualjobs_text_dirty = df[df.fraudulent==0].combined_text

stop_words = set(stopwords.words("english"))
punctuation = list(string.punctuation)
stop_words.update(punctuation)

lemmatizer = WordNetLemmatizer()
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

"""- Inisialisasi stopwords dan menambahkan punctuation kedalam stopwords agar nanti dapat dihapus
- inisialisasi lemmatizer untuk melakukan Lemmatization
- Saya menggunakan Lemmatization dari pada stemming dikarenakan untuk menyempurnakan kata dasarnya
- fungsi WordNetLemmatizer dibuat untuk mendapatkan bagian dari ucapan yang sesuai untuk setiap kata agar lebih akurat
"""

def text_preprocessing(text):
  # Case folding
  text = text.lower()

  # Newline removal (\n)
  text = re.sub(r"\\n", " ",text)

  # Whitespace removal
  text = text.strip()

  # URL removal
  text = re.sub(r"http\S+", " ", text)
  text = re.sub(r"www.\S+", " ", text)

  # Tokenization
  tokens = word_tokenize(text)

  # Stopwords removal
  tokens = [word for word in tokens if word not in stop_words]

  # Lemmatization
  tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]

  # Combining Tokens
  text = ' '.join(tokens)

  return text

"""Fungsi text_preprocessing ini bertujuan untuk melakukan pra-pemrosesan teks atau menghilangkan elemen-elemen yang tidak relevan atau tidak diinginkan dalam teks."""

df['combined_text_processed'] = df['combined_text'].apply(lambda x: text_preprocessing(x))
df.head()

"""Saya membuat 1 kolum baru bernama `combined_text_processed` untuk menyimpan data yang sudah dibersihkan, dapat dilihat perbedaan data yang sudah dibersihkan dengan yang belum dibersihkan."""

fraudjobs_text_cleaned = df[df.fraudulent==1].combined_text_processed
actualjobs_text_cleaned = df[df.fraudulent==0].combined_text_processed

"""### Wordcloud sebelum data dibersihkan"""

plt.figure(figsize = (10,5))
wc = WordCloud(background_color = "white", min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(fraudjobs_text_dirty)))
plt.axis("off")
plt.imshow(wc,interpolation = 'bilinear')

"""Data diatas merupakan teks yang tergolong penipuan dan belum dibersihkan.\
Dapat kita simpulkan berdasarkan visualisasi di atas, frekuensi kata kata yang paling sering muncul adalah experience, work, product, customer, ability, project, position, dll.
"""

plt.figure(figsize = (10,5))
wc = WordCloud(background_color = "white", min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(actualjobs_text_dirty)))
plt.axis("off")
plt.imshow(wc,interpolation = 'bilinear')

"""Data diatas merupakan teks yang tergolong asli dan belum dibersihkan.\
Dapat kita simpulkan berdasarkan visualisasi di atas, frekuensi kata kata yang paling sering muncul adalah will, work, team, client, experience, product, ability, project, dll.

### Wordcloud setelah text preprocessing
"""

plt.figure(figsize = (10,5))
wc = WordCloud(background_color = "white", min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(fraudjobs_text_cleaned)))
plt.axis("off")
plt.imshow(wc,interpolation = 'bilinear')

"""Data diatas merupakan teks yang tergolong penipuan tetapi sudah melewati tahap preprocessing.\
Dapat kita simpulkan berdasarkan visualisasi di atas, frekuensi kata kata yang paling sering muncul adalah process, project, experience, customer service, support, design, ensure, manage, dll.
"""

plt.figure(figsize = (10,5))
wc = WordCloud(background_color = "white", min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(actualjobs_text_cleaned)))
plt.axis("off")
plt.imshow(wc,interpolation = 'bilinear')

"""Data diatas merupakan teks yang tergolong penipuan tetapi sudah melewati tahap preprocessing.\
Dapat kita simpulkan berdasarkan visualisasi di atas, frekuensi kata kata yang paling sering muncul adalah full time, communication skill, customer service, year experience, social medium, fast grow, dll.

## Model Building
"""

max_features = 10000

# Membuat Tokenizer
t = Tokenizer(num_words = max_features)
# fit tokenizer
t.fit_on_texts(df['combined_text_processed'])

encoded_docs = t.texts_to_sequences(df['combined_text_processed'])

joblib.dump(t, './tokinezer_file')

df['word count'] = [len(i.split(' ')) for i in df['combined_text_processed']]

"""> Menambahkan kolom baru bernama `word count` yang berisi jumlah kata dalam setiap teks dalam kolom 'combined_text_processed'."""

sent_length = df['word count'].max()

"""> Menghitung jumlah maksimum kata yang terdapat dalam satu baris."""

embedded_docs = pad_sequences(encoded_docs,padding='pre',maxlen=sent_length)
print(embedded_docs)

"""### Splitting Data"""

y = df['fraudulent'].values
y

y = y.reshape(-1,1)
y.shape

X = np.array(embedded_docs)
X.shape

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state= 101)

"""> Membagi data menjadi dua yaitu data latih dan data uji."""

print("X_train shape: ",X_train.shape)
print("X_test shape : ",X_test.shape )
print("y_train shape: ",y_train.shape)
print("y_test shape : ",y_test.shape)

"""### Model Architecture Definition"""

## Clear Session
seed = 20
tf.keras.backend.clear_session()
np.random.seed(seed)
tf.random.set_seed(seed)

embedding_vector_features=40
model_lstm=Sequential()
model_lstm.add(Embedding(max_features,embedding_vector_features,input_length=sent_length))
model_lstm.add(Bidirectional(LSTM(32, return_sequences= True, kernel_initializer=tf.keras.initializers.GlorotUniform(seed))))
model_lstm.add(Dropout(0.1))
model_lstm.add(Bidirectional(LSTM(32, kernel_initializer=tf.keras.initializers.GlorotUniform(seed))))
model_lstm.add(Dropout(0.1))
model_lstm.add(Dense(1,activation='sigmoid'))
model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model_lstm.summary())

"""### Model Training"""

# Model Training using LSTM
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_lstm_hist = model_lstm.fit(X_train, y_train, epochs=25, batch_size = 64, validation_data=(X_test,y_test),  callbacks=[early_stopping])

"""Saya menggunakan epoch sebanyak 25, namun dikarenakan saya memakai early stopping untuk menghentikan pelatihan secara otomatis jika tidak ada peningkatan kinerja pada metrik validasi selama beberapa epoch berturut-turut. Maka pelatihan akan berhenti ketika dirasa sudah optimal.

### Model Evaluation
"""

plt.plot(model_lstm_hist.history['val_loss'], color='b', label="validation loss")
plt.plot(model_lstm_hist.history['loss'], color='red', label="loss")
plt.title("Loss vs Val-Loss")
plt.xlabel("Number of Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(model_lstm_hist.history['val_accuracy'], color='b', label="validation accuracy")
plt.plot(model_lstm_hist.history['accuracy'], color='red', label="accuracy")
plt.title("Accuracy vs Val-Accuracy")
plt.xlabel("Number of Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""> - Berdasarkan hasil tersebut, dapat disimpulkan bahwa model yang dibuat sudah cukup baik dengan hasil akurasi sebesar 98.25% dan tingkat loss 0.17
> - Berdasarkan hasil dari pengetesan data latih dan data uji juga sangat baik.
> - Model sedikit overfitting namun tidak terlalu terlihat dikarenakan jarak antara latih dan uji tidak terlalu jauh.
"""

model_lstm.evaluate(X_test, y_test)

"""## Conclusion|

> - Berdasarkan hasil tersebut, dapat disimpulkan bahwa model yang dibuat sudah cukup baik dengan hasil akurasi sebesar 97.78% dan tingkat loss 0.08
> - Berdasarkan hasil dari pengetesan data latih dan data uji juga sangat baik.
> - Model sedikit overfitting namun tidak terlalu terlihat dikarenakan jarak antara latih dan uji tidak terlalu jauh.
"""

# saving the model
model_lstm.save("job_prediction.h5")
!tar -zcvf job_prediction_model.tgz "job_prediction.h5"

pip install streamlit

import streamlit as st

# Judul aplikasi
st.title("Capstone Project Kelompok 2")

# Deskripsi atau penjelasan aplikasi
st.write("Ini adalah aplikasi Streamlit untuk Capstone Project Kelompok 2.")

# Menambahkan kode dari notebook asli Anda
# Contoh:
# st.write("Hasil analisis data:")
# st.dataframe(df)

# Anda perlu menambahkan lebih banyak kode di sini sesuai dengan konten notebook Anda

streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py

streamlit run Kelompok_2_Capstone_Project_Final.py

pip install nbformat

import nbformat

# Load the notebook
notebook_path = "/mnt/data/Kelompok_2_Capstone_Project_Final.ipynb"
notebook = nbformat.read(notebook_path, as_version=4)

# Convert notebook to Python script
python_script = nbformat.writes(notebook, version=4)

# Save the converted Python script
python_script_path = notebook_path.replace(".ipynb", ".py")
with open(python_script_path, "w") as f:
    f.write(python_script)

python_script_path